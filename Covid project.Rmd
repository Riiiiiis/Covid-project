---
title: "Covid project"
author: "Kristian Riis Hansen"
date: '2022-09-06'
output: html_document
---
Libraries:
```{r}
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
library(knitr)
library(stringr)

```

working directory
```{r}
setwd("C:/Users/Krist/Desktop/PiB/Variants and mutation rate in SARS-cov2")
getwd()

```


Loading data: 
```{r}


tidy_300K <- readRDS("metadata_snpeff_tidy_300K_downsampled.RDS")
mutations_tidy_300K <- readRDS("mutations_snpeff_annotated_tidy_300K_downsampled.rds")

```


```{r}

x <- read_tsv("NC_045512.2.fasta")
reference_genome <- strsplit(x$`>NC_045512.2 Severe acute respiratory syndrome coronavirus 2 isolate Wuhan-Hu-1, complete genome`[1], "") %>% unlist()
rm(x)
table(reference_genome) %>% knitr::kable()
 

ref_genome <- tibble(base = reference_genome) %>% mutate(position=row_number())
```



Merging gene coloumn from tidy_300k to mutation dataset:


we want to detect the selective sweep
we found a method for that in one of the articles - so we count the mutations for each positions and we got that around position 22.500 there are significantly more datapoints so we want to analyse that specific region and check the certain gene which might be responsible for the sweep.


We want to merge the datasets so we have dates, country, GC-content.

```{r}

test_tidy_df <- (tidy_300K[,c(1,10,13,31)])

test_mutation_df <- test_mutation_df %>% group_by(id)


test_merged_df <- left_join(tidy_300K[,c(1,10,13,23,31)], mutations_tidy_300K, by = c("id"="id"))

#Not that many samples_: Maybe do it for the whole of world and see.

#Join the wuhan genome to the all_merged:
#left <- test_merged_df

#right <- ref_genome 

all_merged_with_wuhan <- left_join(test_merged_df, ref_genome, by = "position")


#Filtering for only VOC's
all_merged_with_wuhan <- all_merged_with_wuhan %>% filter(str_detect(Variant, "^VOC"))

#Shortening the VOC variable names and filtering for ref_base only A|T|C|G


all_merged_with_wuhan <- all_merged_with_wuhan %>% mutate(variant = str_extract(all_merged_with_wuhan$Variant, "[^(]+")) %>% filter(ref_base == c("A","C","G","T"))



all_merged_with_wuhan$Pairs <- paste(all_merged_with_wuhan$base, all_merged_with_wuhan$variant_base)



#We reformat the Collection date, so it becomes an actual date.
all_merged_with_wuhan$`Collection date` <- ymd(all_merged_with_wuhan$`Collection date`)


unique(all_merged_with_wuhan$variant_base)

#We remove all variant bases that are not only one nucleotide.
bases=c("A","T","C","G")
all_merged_with_wuhan <- all_merged_with_wuhan %>% filter(variant_base == bases) 
  

```



Looking at GC-content over time in Denmark.

```{r}

ggplot(data=all_merged_with_wuhan, aes(x=all_merged_with_wuhan$`Collection date`, y=all_merged_with_wuhan$"GC-Content")) + 
         geom_point() +
        theme(axis.text.x=element_text(angle=90)) + ylab("GC content")
      



```

We are interested in finding out, whether mutations have accumulated at certain positions along the genome, we therefore plot amount of mutations given position. Furthermore, we can illustrate which gene these mutations are happening on.

```{r}

all_merged_with_wuhan %>% group_by(position) %>%
  ggplot()+
  geom_histogram(mapping= aes(x=position,fill=variant))+
  ggtitle("Mutations over the whole genome")



#With filtered counts (limited)
#all_merged_with_wuhan %>% group_by(position) %>% mutate(count=n()) %>% filter(count <=5) %>% 
#  ggplot()+
#  geom_histogram(mapping=aes(x=position))


#It looks like its more stable over the whole genome then.
```

Mutations over time for the "5 main lineages"


```{r}





#tidy_300K_test <- tidy_300K_test %>% mutate(pango_lineage = ifelse(pangolin_lineage %in% interesting_lineages, pangolin_lineage, "Other lineages"))


```


GC content of the 4-5 lineages of interest over time.

```{r}



tidy_300K_test %>%
  group_by(id) %>%
  filter(str_detect(Variant, "^VOC")) %>% #This is saying the Variants only starting with "VOC" 
  ggplot(tidy_300K_test, mapping = aes(x=`Collection date`,y=`GC-Content`, color=Variant)) +
  geom_point(size = 2) +
  geom_smooth(aes(color=Variant), method = "lm", se = F) +
  theme(axis.text.x=element_text(angle=90)) +
  theme(legend.title = element_text(size = 4),
         legend.text = element_text(size = 4)) +
  ggtitle("GC-Content over time for the common lineages") +
  NULL



all_merged_with_wuhan %>% ggplot(all_merged_with_wuhan,mapping=aes(x=`Collection date`,y=`GC-Content`)) +
  geom_point(aes(color=variant))+
  geom_smooth(aes(color=variant),method = "lm", se=F) +
  ggtitle("GC-Content over time for the Variants of Concern")
```



dn/ds for each lineage

THIS IS NOT THAT COOL OR IMPORTANT.
```{r}

tidy_300K <- tidy_300K %>% mutate(DnDs = (count_N/count_S))

tidy_300K <- tidy_300K %>% mutate(variant = str_extract(tidy_300K$Variant, "[^(]+"))


tidy_300K %>% 
  group_by(id) %>% 
  filter(str_detect(variant, "^VOC")) %>% #This is saying the Variants only starting with "VOC" 
  ggplot(tidy_300K, mapping = aes(x=`Collection date`,y=DnDs, color=variant)) +
  geom_point() +
  ggtitle("DnDs ratio")+
  facet_wrap( ~ variant)
  


```


We want to find out whether the mutations are often G/C or otherwise. To do so we look at the reference genome: Wuhan1 and by using a selfwritten function, we use the mutation dataset to see where aswell as which mutation. So 1. we can see which mutations are most prone and also in which regions. We do this because we hypothesize that there are more mutations in the region for immunerecognision associated genes.





THIS SHOULD PROBABLY BE DELETED ASWELL
```{r}

  
  #Just for removing a column:
#dk_merged_with_wuhan <- subset(dk_merged_with_wuhan, select = -newCol)

#Adding Variants to the dk_merged_with_wuhan with merged:

right <- tidy_300K %>% select(Variant,id)
left <- dk_merged_with_wuhan

dk_merged_with_wuhan <- left_join(left,right, by = "id")

dk_merged_with_wuhan$`Collection date` <- ymd(dk_merged_with_wuhan$`Collection date`)
#Now we go into pyhton and load the "dk_merged_wuhan.csv"


#We filter so variant base cannot be base either.
dk_merged_with_wuhan <- dk_merged_with_wuhan %>% filter(variant_base != base)



write.csv(dk_merged_with_wuhan,"dk_merged_wuhan.csv")


dk_wuhan_statcount <- read.csv("dk_wuhan_statcount.csv")
dk_wuhan_statcount_with_Variant <- read.csv("dk_wuhan_statcount_with_Variant.csv")



```


Wrangling dataset with base change counts:

We want to produce a plot showing the occurences of changes for each position. Which variant base is most likely to shift to. We do this to get an understanding and insight into the possibility that different regions of the genome might be under different selective pressures. Coupled with the GC-content.
```{r}

#unique(all_merged_with_wuhan$variant_base)
#only A,T,C and G

all_merged_with_wuhan %>% group_by(position,Pairs,variant,variant_base) %>% summarise(count=n()) %>% filter(count <=5) %>% filter(between(position,21500,22500)) %>%   ggplot(aes(x=position,y=count,color=variant_base)) +
  geom_point() +
  facet_wrap(~ variant)
```

Frequency plots: one for the general tendency and then for some specific regions:


```{r}

#Count plot for each change over the whole genome:

all_merged_with_wuhan %>% group_by(variant,Pairs) %>% summarise(count=n())


all_merged_with_wuhan %>% group_by(variant,Pairs) %>% summarise(count=n()) %>% 
  ggplot(all_merged_with_wuhan,mapping= aes(x=Pairs,y=count,color=variant))+
           geom_point() +
  ggtitle("Counts of changes over the whole genome")


```


Plot showing aggregated counts of new mutations (distinguished by Pairs), in range of the Spike gene.
This is to give an idea about which Basepairs are most often mutated, and if it differs between variants.
```{r}

all_merged_with_wuhan %>% filter(between(position,21500,22150)) %>% group_by(variant, Pairs,position) %>% summarise(n=n()) %>% filter(n <= 5) %>% aggregate(n ~ Pairs + variant,FUN = sum) %>% 
  ggplot(aes(x=Pairs,y=n,color=variant))+
  geom_point() +
  ggtitle("Aggregated counts of new mutations in range of Spike Gene")
```





L452R over timeline:
NOT IMPORTANT

```{r}


all_merged_with_wuhan %>% group_by(variant,aa_change,`Collection date`) %>% filter(aa_change == "L452R") %>% summarise(n=n()) %>% ggplot(aes(x=`Collection date`,y=n,color=variant)) +
  geom_point()
                                                                                                                         


```


Where is the spike gene: around 21500 -- 22150


```{r}
syn <- "synonymous_variant"

#dk_merged_with_wuhan %>% mutate(Type = ifelse(effect %in% syn, effect, "Non-syn"))




all_merged_with_wuhan %>% mutate(Type = ifelse(effect %in% syn, effect, "Non-syn")) %>% filter(between(position,21500,22150)) %>% group_by(variant,position,Type) %>% summarise(n=n()) %>% filter(n <= 5) %>% ggplot()+
  geom_point(aes(x=position,y=n,color=Type)) +
  facet_wrap(~ variant)



```

Maybe a plot showing the above, but with all the overlaps, all the low frequency mutations that are present in all the variants.


```{r}


#HAVENT RERUN THIS YET WITH fixed filter.
test <- all_merged_with_wuhan %>% filter(between(position,21500,22150)) %>% group_by(position,variant) %>% summarise(n=n())
  
test
write.csv(test,"forlooped test.csv")
#Python script to be shown here?????
Mutation_overlap <- read.csv("mutation_overlap.csv")


#

Mutation_overlap %>% filter(between(position, 21500,22150)) %>% group_by(variant, position) %>% filter(n <= 5) %>% 
  ggplot(aes(x=position, y=n,color = variant))+
  geom_point()+
    ggtitle("Low frequency mutations on the NTD & RBD regions")


Mutation_overlap %>% filter(between(position, 21500,22150)) %>% group_by(variant, position) %>% filter(n <= 5) %>% 
  ggplot(aes(x=position, y=n,color = mutation_overlap))+
  geom_point()+
    ggtitle("Low frequency mutations on the NTD & RBD regions")







```




A plot showing all the positions without a mutation: for all different VOC. they show the same pattern?
```{r}

#BERNADETT HAS THIS


```






Contingency tables:

We hypothesize that variants might have different selective pressures --> especially on the region of the Spike gene, that is associated with human immunoresponse. Therefore we look at the number of mutations for all available variants in denmark on this position of the genome. They might differ in which basepair mutations are happening (cg depletion at different levels?). However we have to remember that there are alot less counts for some of the variants.



Further doing statistics on the contingency tables:
With chi squared test: Check if column variables are independent of row variables.




Contingency tables with only filtered for 5 or less counts
```{r}

#NEW PROBLEM: SOLVED
contingency_table_count <- all_merged_with_wuhan %>% filter(between(position,21500,22150)) %>% group_by(variant, Pairs,position) %>% summarise(n=n()) %>% filter(n<= 5)



#Aggregate unique positions for Pairs and variants.

contingency_table_count <- aggregate(n~ Pairs+variant, data= contingency_table_count,FUN=sum)

#ALso quickly check how many mutations there are on RBD and NTD.

contingency_table_count <- contingency_table_count %>% spread(Pairs,n)


contingency_table_count[is.na(contingency_table_count)]=0
```


Comparing the contingency table with the whole genome for limited counts of mutations:
```{r}

all_merged_with_wuhan %>% filter(between(position,21500,22150)) %>% group_by(variant, Pairs,position) %>% summarise(n=n()) %>% filter(n <= 5) %>% aggregate(n ~ Pairs + variant,FUN = sum) %>% spread(Pairs,n)

#This still has NA's though.


#Make the chisq for counts table.
```
Trying to create contingency tables that contain percentages instead, still aggregated counts.
```{r}

prop_contingency_table <- all_merged_with_wuhan %>% filter(between(position,21500,22150)) %>% group_by(variant, Pairs,position) %>% summarise(n=n()) %>% filter(n <= 5) %>% aggregate(n ~ Pairs + variant,FUN = sum) %>% group_by(variant) %>% #group by variant so that propsums become relative to variant
  mutate(prop=n/sum(n)) %>%  ungroup() %>% #ungroup again
  select(variant,prop,Pairs) %>% spread(Pairs, prop) #select only these 3 for using the spread function


prop_contingency_table[is.na(prop_contingency_table)]=0


(rowSums(prop_contingency_table[,-1]))
#All the rows sum to 1. Which is great.

prop_contingency_table



```


Doing stats on prop_contingency table


```{r}
df_test <- data.frame(AC= numeric(0),AG=numeric(0),AT=numeric(0),CA=numeric(0),CG=numeric(0),CT=numeric(0), GA=numeric(0), GC=numeric(0), GT=numeric(0), TA=numeric(0), TC=numeric(0),TG=numeric(0))


contingency_table_count

AC <- chisq.test(contingency_table_count$variant,contingency_table_count$`A C`)



AG <- chisq.test(contingency_table_count$variant,contingency_table_count$`A G`,simulate.p.value = TRUE)

AT <- chisq.test(prop_contingency_table$variant,prop_contingency_table$`A T`)


CA <- chisq.test(prop_contingency_table$variant,prop_contingency_table$`C A`)

CG <- chisq.test(prop_contingency_table$variant,prop_contingency_table$`C G`)

CT <- chisq.test(prop_contingency_table$variant,prop_contingency_table$`C T`)

GA <- chisq.test(prop_contingency_table$variant,prop_contingency_table$`G A`)

GC <- chisq.test(prop_contingency_table$variant,prop_contingency_table$`G C`)

GT <- chisq.test(prop_contingency_table$variant,prop_contingency_table$`G T`)

TA <- chisq.test(prop_contingency_table$variant,prop_contingency_table$`T A`)

TC <- chisq.test(prop_contingency_table$variant,prop_contingency_table$`T C`)

TG <- chisq.test(prop_contingency_table$variant,prop_contingency_table$`T G`)


df_pairs <- data.frame(AC$p.value, AG$p.value, AT$p.value, CA$p.value, CG$p.value, CT$p.value, GA$p.value, GC$p.value, GT$p.value, TA$p.value,TC$p.value, TG$p.value)

TC$p.value
TA$p.value
TG$p.value


chisq.test(prop_contingency_table$variant, prop_contingency_table$`T C`)

prop_contingency_table


df_test[nrow(df_test)] = c(AC$p.value, AG$p.value, AT$p.value, CA$p.value, CG$p.value, CT$p.value, GA$p.value, GC$p.value, GT$p.value, TA$p.value,TC$p.value, TG$p.value)

?chisq.test()

chisq.test(table(prop_contingency_table$variant,prop_contingency_table$`A C`))

chisq.test(table(prop_contingency_table$variant,prop_contingency_table$`A G`))


chisq.test(table(prop_contingency_table$variant,prop_contingency_table$`G C`))




#THIS IS WHAT WE WANT
contingency_table_count
chisq.test(contingency_table_count[,-1])


#We get a significant p-value, which means that we reject the H0: that there is no association between two or more categorical variables. Therefore there is an association between the variables (Some biological force perhaps)

library(chisq.posthoc.test)


chisq.posthoc.test(contingency_table_count[,-1],method= "bonferroni")

#Read up on how posthoc.test results should be understood.



#
```


```{r}

#Actual counts.
all_merged_with_wuhan %>% filter(between(position,21500,22150)) %>% group_by(variant, Pairs,position) %>% summarise(n=n()) %>% filter(n <= 5) %>% aggregate(n ~ Pairs + variant,FUN = sum) %>% 
  ggplot()+
    geom_col(aes(x=Pairs,y=n,fill=variant))

#Proportionised for each variant, so it sums to 1 for each.
all_merged_with_wuhan %>% filter(between(position,21500,22150)) %>% group_by(variant, Pairs,position) %>% summarise(n=n()) %>% filter(n <= 5) %>% aggregate(n ~ Pairs + variant,FUN = sum) %>% group_by(variant) %>% mutate(prop=n/sum(n)) %>% 
  ggplot()+
    geom_col(aes(x=variant,y=prop,fill=Pairs))

#Shift around response

```









```{r}


all_merged_with_wuhan %>% group_by(id) %>% mutate(mutations=n()) %>% 
  ggplot(mapping=aes(x=`Collection date`,y=mutations,color=variant))+
  geom_point()+
  geom_smooth(aes(color=variant),method="lm",se=F) +
  ylim(0,20)


  

```

